{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XJVweSxrgBB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4ZF9-EDwsue"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syqpzqVha3Mo",
        "outputId": "ab24c1b3-6467-42c2-849f-00dcb78ec26d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ht_2giz0a4Bk"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ro5wjfua6CC"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the paths to your zip files on Google Drive\n",
        "beds_zip = '/content/drive/My Drive/beds.zip'\n",
        "sofa_zip = '/content/drive/My Drive/sofa.zip'\n",
        "chairs_zip = '/content/drive/My Drive/chairs.zip'\n",
        "tables_zip = '/content/drive/My Drive/tables.zip'\n",
        "shelving_zip = '/content/drive/My Drive/shelving.zip'\n",
        "wardrobe_zip = '/content/drive/My Drive/wadrobe.zip'\n",
        "\n",
        "# Unzip each file into its corresponding folder\n",
        "with zipfile.ZipFile(beds_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/beds')\n",
        "\n",
        "with zipfile.ZipFile(sofa_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/sofa')\n",
        "\n",
        "with zipfile.ZipFile(chairs_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/chairs')\n",
        "\n",
        "with zipfile.ZipFile(tables_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/tables')\n",
        "\n",
        "with zipfile.ZipFile(shelving_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/shelving')\n",
        "\n",
        "with zipfile.ZipFile(wardrobe_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/wadrobe')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_0BhJwJlAwP"
      },
      "outputs": [],
      "source": [
        "beds_folder = '/content/beds'\n",
        "sofa_folder = '/content/sofa'\n",
        "chairs_folder = '/content/chairs'\n",
        "tables_folder = '/content/tables'\n",
        "shelving_folder = '/content/shelving'\n",
        "wardrobe_folder = '/content/wadrobe'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn8DFn-kw5Hh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xRzGNCXz9b3"
      },
      "source": [
        "IMAGE FOLDER EXTRACTION FROM DRIVE\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx9DrW_LuAYD"
      },
      "outputs": [],
      "source": [
        "# Function to load images from a folder and create a DataFrame\n",
        "def load_images(folder_path):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is not None:  # Check if image was loaded successfully\n",
        "                image_paths.append(image_path)\n",
        "    return pd.DataFrame({'image_path': image_paths})\n",
        "\n",
        "# Load images from each folder\n",
        "beds_df = load_images(beds_folder)\n",
        "sofa_df = load_images(sofa_folder)\n",
        "chairs_df = load_images(chairs_folder)\n",
        "tables_df = load_images(tables_folder)\n",
        "shelving_df = load_images(shelving_folder)\n",
        "wadrobe_df = load_images(wardrobe_folder)\n",
        "\n",
        "\n",
        "# Combine DataFrames\n",
        "combined_df = pd.concat([beds_df, sofa_df, chairs_df, tables_df, shelving_df, wadrobe_df])\n",
        "print(combined_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7QeDPrjYjlBS"
      },
      "outputs": [],
      "source": [
        " from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v-MoxrvwzYj"
      },
      "source": [
        "DUPLICATION REMOVAL\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KPhA-jzuFNs"
      },
      "outputs": [],
      "source": [
        "# Remove duplicate images based on image content\n",
        "def remove_duplicates(df):\n",
        "    image_hashes = []\n",
        "    unique_df = pd.DataFrame()\n",
        "    for index, row in df.iterrows():\n",
        "        image_path = row['image_path']\n",
        "        image = cv2.imread(image_path)\n",
        "        image_hash = hash(image.tobytes())\n",
        "        if image_hash not in image_hashes:\n",
        "            image_hashes.append(image_hash)\n",
        "            unique_df = pd.concat([unique_df, row.to_frame().T], ignore_index=True)\n",
        "    return unique_df\n",
        "\n",
        "# Remove duplicates from the combined DataFrame\n",
        "combined_df = remove_duplicates(combined_df)\n",
        "print(combined_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS6x_CyWwtg1"
      },
      "source": [
        "IMAGE PROCESSING\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xalufKF5ucKw"
      },
      "outputs": [],
      "source": [
        "def process_image(image_path, image_size=(80, 80)):\n",
        "    try:\n",
        "        img = Image.open(image_path).convert('RGB')  # Open and convert to RGB\n",
        "        # Resize the image\n",
        "        img = img.resize(image_size)\n",
        "        img_array = np.array(img)  # Convert to a numpy array\n",
        "        img_array = img_array.flatten()  # Flatten the 3D image to 1D\n",
        "        return img_array\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTFnf6FrmNBk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Apply the process_image function to the 'image_path' column of the DataFrame\n",
        "combined_df['processed_image'] = combined_df['image_path'].apply(process_image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ3qsyhvvavk"
      },
      "outputs": [],
      "source": [
        "def color_edge_detection(image, image_size=(80,80)):\n",
        "  try:\n",
        "    img=img.resize(image_size)\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    img_blur = cv2.GaussianBlur(img_gray, (3,3), 0)\n",
        "    # Apply Canny edge detection\n",
        "    edges = cv2.Canny(gray, 50, 150)\n",
        "    img_array=edges.flatten()\n",
        "\n",
        "\n",
        "    return img_array\n",
        "\n",
        "  except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "combined_df['processed_edges'] = combined_df['image_path'].apply(process_image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbnDMbT2qcMh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FoFI9z4vhUI"
      },
      "outputs": [],
      "source": [
        "combined_df = combined_df.applymap(lambda x: [x] if isinstance(x, int) else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuyCwAJgLrry"
      },
      "outputs": [],
      "source": [
        "combined_df['processed_image']=combined_df['processed_image'].dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbhQWmIpLyGA"
      },
      "outputs": [],
      "source": [
        "combined_df = combined_df[combined_df['processed_image'].apply(lambda x: not isinstance(x, float))]\n",
        "new_df = pd.DataFrame(combined_df['processed_image'].to_list())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1keYPZUvyVx"
      },
      "source": [
        "PCA\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bdntwX8RW3l"
      },
      "outputs": [],
      "source": [
        "combined_df['processed_edges']=combined_df['processed_edges'].dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hlme9W1Il_Y5"
      },
      "outputs": [],
      "source": [
        "combined_df = combined_df[combined_df['processed_edges'].apply(lambda x: not isinstance(x, float))]\n",
        "new_df_edges = pd.DataFrame(combined_df['processed_edges'].to_list())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Y7euWHLmR1B"
      },
      "outputs": [],
      "source": [
        "new_df_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hig1udhewgLk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor, ViTMAEModel\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n",
        "model = ViTMAEModel.from_pretrained(\"facebook/vit-mae-base\")\n",
        "\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "last_hidden_states = outputs.last_hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtm-aWacw6XW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor, ViTMAEModel\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n",
        "model = ViTMAEModel.from_pretrained(\"facebook/vit-mae-base\")\n",
        "\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "last_hidden_states = outputs.last_hidden_state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQKdjUZ7x01V"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoF0MsXdx1HA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBnfoAt7xi56"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.model = ViTMAEModel.from_pretrained(\"facebook/vit-mae-base\")\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs.pop('image_name', None)\n",
        "\n",
        "        outputs = self.model(**inputs)\n",
        "        return torch.sum(outputs.last_hidden_state, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4ufZtY5Vx3kM",
        "outputId": "fd8e0326-f075-4530-afd3-96e2ba652df8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing image /content/chairs/image_1698.jpg: Unsupported number of image dimensions: 2\n",
            "Error processing image /content/tables/image_1619.jpg: Unsupported number of image dimensions: 2\n",
            "Error processing image /content/tables/image_1717.jpg: Unsupported number of image dimensions: 2\n",
            "                     image_path  \\\n",
            "0  /content/sofa/image_1257.jpg   \n",
            "1   /content/sofa/image_396.jpg   \n",
            "2  /content/sofa/image_1413.jpg   \n",
            "3   /content/sofa/image_971.jpg   \n",
            "4  /content/sofa/image_1305.jpg   \n",
            "\n",
            "                                            features  \n",
            "0  [-0.6886653900146484, -7.901733875274658, -6.1...  \n",
            "1  [-6.35953950881958, 0.3591250777244568, 0.0132...  \n",
            "2  [-8.57598876953125, 1.2275011539459229, -0.181...  \n",
            "3  [-4.2629780769348145, 1.2356879711151123, -1.3...  \n",
            "4  [-7.434243202209473, 0.06401626765727997, 1.43...  \n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoProcessor, ViTMAEModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# Define the FeatureExtractor class\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.model = ViTMAEModel.from_pretrained(\"facebook/vit-mae-base\")\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs.pop('image_name', None)\n",
        "        outputs = self.model(**inputs)\n",
        "        return torch.sum(outputs.last_hidden_state, dim=1)\n",
        "\n",
        "# Load the processor\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/vit-mae-base\")\n",
        "\n",
        "# Function to process each image and extract features\n",
        "def extract_features_from_df(row, feature_extractor, processor):\n",
        "    try:\n",
        "        # Load the image from the path\n",
        "        image = Image.open(row['image_path'])\n",
        "\n",
        "        # Preprocess the image\n",
        "        inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "        # Extract features\n",
        "        with torch.no_grad():  # For inference\n",
        "            features = feature_extractor(inputs)\n",
        "\n",
        "        # Convert tensor to list (or numpy array) if you want to store it in a DataFrame\n",
        "        return features.squeeze().tolist()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {row['image_path']}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example DataFrame with image paths\n",
        "df = pd.DataFrame(combined_df['image_path'])\n",
        "# Instantiate the FeatureExtractor\n",
        "feature_extractor = FeatureExtractor()\n",
        "\n",
        "# Apply the feature extraction to each row\n",
        "df['features'] = df.apply(lambda row: extract_features_from_df(row, feature_extractor, processor), axis=1)\n",
        "\n",
        "# Check the output DataFrame with extracted features\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Okvj4toOzvDv",
        "outputId": "a71b4493-5154-4f12-aa44-343a37d5b24c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df.iloc[0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "44nZWYV48LM3",
        "outputId": "4b70b437-7eda-45d8-bd89-e661aeee9c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     image_path  \\\n",
            "0  /content/sofa/image_1257.jpg   \n",
            "1   /content/sofa/image_396.jpg   \n",
            "2  /content/sofa/image_1413.jpg   \n",
            "3   /content/sofa/image_971.jpg   \n",
            "4  /content/sofa/image_1305.jpg   \n",
            "\n",
            "                                            features         0         1  \\\n",
            "0  [-0.6886653900146484, -7.901733875274658, -6.1... -0.688665 -7.901734   \n",
            "1  [-6.35953950881958, 0.3591250777244568, 0.0132... -6.359540  0.359125   \n",
            "2  [-8.57598876953125, 1.2275011539459229, -0.181... -8.575989  1.227501   \n",
            "3  [-4.2629780769348145, 1.2356879711151123, -1.3... -4.262978  1.235688   \n",
            "4  [-7.434243202209473, 0.06401626765727997, 1.43... -7.434243  0.064016   \n",
            "\n",
            "          2         3          4          5         6         7  ...  \\\n",
            "0 -6.199350  7.763186 -32.192436  -7.438576  3.239686  3.725930  ...   \n",
            "1  0.013264 -5.996150 -24.487288  -8.854685  6.449086  2.317488  ...   \n",
            "2 -0.181056 -1.323359 -27.304140  -4.040198  3.858768  2.606106  ...   \n",
            "3 -1.352839 -0.813753 -19.529764 -21.539541  7.131307 -1.747911  ...   \n",
            "4  1.432333  1.956686 -28.034672  -1.495188  4.457454  3.316759  ...   \n",
            "\n",
            "        758       759       760        761       762        763       764  \\\n",
            "0 -1.971586 -3.529150  0.712962  -3.664695  1.966313 -12.507627 -1.858355   \n",
            "1 -4.214735 -4.114333 -5.298071  -5.541027  4.241629  -5.009718 -7.396216   \n",
            "2 -7.145366 -8.766317 -2.739997  -9.374910  4.131849  -9.552088 -9.987898   \n",
            "3 -4.409951 -2.711349 -6.364720  -5.852399  3.070816  -8.895936 -3.553301   \n",
            "4 -5.470455 -9.863923 -1.132439 -11.571935  4.476186  -8.394115 -6.705567   \n",
            "\n",
            "        765        766       767  \n",
            "0 -1.223877 -11.675611  8.597697  \n",
            "1 -1.427462 -12.307479  6.231584  \n",
            "2  0.318413 -10.715476 -5.112961  \n",
            "3 -5.888100  -8.268599  3.545832  \n",
            "4  3.116898 -11.054033 -2.476945  \n",
            "\n",
            "[5 rows x 770 columns]\n"
          ]
        }
      ],
      "source": [
        "# Drop rows where 'features' column contains NaN or None\n",
        "df = df.dropna(subset=['features'])\n",
        "\n",
        "# Ensure 'features' column contains lists (or valid feature data)\n",
        "df = df[df['features'].apply(lambda x: isinstance(x, list))]\n",
        "\n",
        "# Convert the list of features into a new DataFrame (expand each list element into its own column)\n",
        "features_df = pd.DataFrame(df['features'].to_list())\n",
        "\n",
        "# Reset the index of both DataFrames to ensure unique indices\n",
        "df = df.reset_index(drop=True)\n",
        "features_df = features_df.reset_index(drop=True)\n",
        "\n",
        "# Concatenate both DataFrames along the columns\n",
        "df = pd.concat([df, features_df], axis=1)\n",
        "\n",
        "# Check the updated DataFrame\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2qQ6mXYs8_Dj",
        "outputId": "e3f96d9e-13f7-4b81-b687-18fd987b2fe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cluster\n",
            "-1    8360\n",
            " 0      28\n",
            " 3      21\n",
            " 1      17\n",
            " 5      16\n",
            " 8      13\n",
            " 6      10\n",
            " 2       7\n",
            " 4       6\n",
            " 7       5\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize features for DBSCAN (important for distance-based clustering algorithms)\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(df['features'].to_list())  # Assuming 'features' contains the extracted features\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=12.5, min_samples=5)  # Adjust 'eps' and 'min_samples' based on your data\n",
        "df['cluster'] = dbscan.fit_predict(scaled_features)\n",
        "\n",
        "# Check the cluster labels\n",
        "print(df['cluster'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IrA-ex7mps8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming df is your DataFrame and 'features' contains the extracted features\n",
        "# Replace 'df' with the actual DataFrame you are using\n",
        "\n",
        "# Standardize features for DBSCAN (important for distance-based clustering algorithms)\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(df['features'].to_list())  # Convert 'features' column to list before scaling\n",
        "\n",
        "# Define a range of eps values (from 10 to 20)\n",
        "eps_values = np.arange(10, 21, 1)\n",
        "\n",
        "# Initialize lists to store results\n",
        "num_clusters = []\n",
        "silhouette_scores = []\n",
        "\n",
        "for eps in eps_values:\n",
        "    # Fit DBSCAN model with the current eps\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
        "    df['cluster'] = dbscan.fit_predict(scaled_features)  # Predict clusters and assign to DataFrame\n",
        "\n",
        "    # Get the labels and calculate the number of clusters (ignoring noise points)\n",
        "    labels = df['cluster']\n",
        "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    num_clusters.append(n_clusters)\n",
        "\n",
        "    # Calculate silhouette score if there's more than 1 cluster\n",
        "    if n_clusters > 1:\n",
        "        sil_score = silhouette_score(scaled_features, labels)\n",
        "        silhouette_scores.append(sil_score)\n",
        "    else:\n",
        "        silhouette_scores.append(-1)  # Invalid silhouette score for single cluster or no clusters\n",
        "\n",
        "# Plot the number of clusters vs. eps (Elbow plot)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(eps_values, num_clusters, marker='o', label='Number of Clusters')\n",
        "plt.title('Elbow Plot for DBSCAN (Number of Clusters vs. eps)')\n",
        "plt.xlabel('eps')\n",
        "plt.ylabel('Number of Clusters')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the silhouette score vs. eps\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(eps_values, silhouette_scores, marker='o', color='green', label='Silhouette Score')\n",
        "plt.title('Silhouette Score vs. eps')\n",
        "plt.xlabel('eps')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Apply DBSCAN with eps = 18\n",
        "dbscan = DBSCAN(eps=18, min_samples=5)\n",
        "df['cluster'] = dbscan.fit_predict(scaled_features)\n",
        "\n",
        "# Check the cluster labels (number of samples in each cluster)\n",
        "print(df['cluster'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8UTJtkx-Cub"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "random_index = random.choice(df.index)\n",
        "random_image_features = df.loc[random_index, 'features']\n",
        "random_cluster = df.loc[random_index, 'cluster']\n",
        "\n",
        "\n",
        "same_cluster_df = df[df['cluster'] == random_cluster]\n",
        "cluster_features = np.array(same_cluster_df['features'].tolist())\n",
        "\n",
        "similarities = cosine_similarity([random_image_features], cluster_features).flatten()\n",
        "\n",
        "most_similar_indices = similarities.argsort()[-4:-1]\n",
        "\n",
        "selected_image_path = df.loc[random_index, 'image_path']\n",
        "similar_image_paths = same_cluster_df.iloc[most_similar_indices]['image_path'].tolist()\n",
        "\n",
        "def display_images(image_paths, title):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for i, img_path in enumerate(image_paths):\n",
        "        img = Image.open(img_path)\n",
        "        plt.subplot(1, len(image_paths), i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "display_images([selected_image_path], \"Selected Image\")\n",
        "\n",
        "display_images(similar_image_paths, \"3 Most Similar Images\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}